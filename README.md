# -Stochastic-rate-model-analytical-predictions-and-numerical-tests

Neural networks are extensively utilized for classifying objects into various categories. These networks, in their feedforward design, comprise a series of computational nodes known as neurons, organized into successive layers and interconnected through weighted connections that are appropriately adapted. The classification process begins with the input of information into the first layer, and through successive modulations of this input signal, a final decision is reached in the output layer. Building on this foundation, our research aims to develop and evaluate a novel classification algorithm rooted in dynamic systems theory. This approach involves guiding the evolution of the model toward different attractors, which are initially deterministic and then stochastic. Specifically, we have investigated a stochastic variant of the rate model as presented by R. Kim et al. in PNAS 116.45 (2019)[1]. The goal of this study is twofold: to numerically and analytically characterize the asymptotic behavior of these systems in the context of varying noise source characteristics. The solutions derived from this research can potentially lead to the development of a noise-assisted classifier, following the conceptual framework outlined above. Exploring these implications fall however outside the scope of this work.

My aim is to construct the theoretical framework by analytically studying the probability of fluctuations that arise at late time in stochastic scenarios. Statistical analysis of stochastic fluctuations around fixed points aims to provide a more complete understanding of learning dynamics in neural networks. By building on these fundamental tools, the aim of the research group is to develop an innovative approach to learning that sits at the interface with the theory of dynamical systems.
My work began with an investigation into the modelâ€™s evolution towards different attractors, starting with deterministic and then stochastic scenarios. Furthermore, my objective is to characterize both numerically and analytically the asymptotic behavior of these systems under various noise characteristics.

I investigated a rate model composed of three interacting elements subject to a stochastic drive with a given correlator. The model was pre-modified to introduce an asymptotic attractor. Our objective was to derive an analytical expression for the asymptotic probability distribution under the linear noise approximation, assuming the magnitude of the imposed noise to be sufficiently small. I then compared these analytical predictions with direct stochastic simulations conducted using the Euler-Maruyama scheme.


The next step will be to generalize the diffusion matrix, B, and then to embed the learnable parameters in the el- ements of the adjacency matrix, A, and to study the noise in the diffusion matrix, B, for its impact on training. The future vision is that these dynamic systems, unlike traditional neural networks, could increase their resilience to perturbations by learning in a stochastic environment.
Noise has been the focal point of this report, and the next objective will be to estimate its impact during training. The intriguing aspect is that robustness will be instilled within the dynamics of the system. Incorporating it will create neural networks that are more resistant to attack, as noise could promote generalization and learning. In conclusion, during this internship, by examining the distribution of fluctuations in a stochastic dynamic model, we have laid the foundation for a stochastic learning model with dynamic systems.
